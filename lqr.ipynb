{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/flow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1600px;height:750px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[1.], [1.]])  # 2x1 vector \n",
    "\n",
    "A = tf.constant(np.array([[-0.5, 0], [0, -0.6]]), dtype=tf.float32, name='A')  # system dynamics\n",
    "B = tf.constant(np.array([[1.], [1.]]), dtype=tf.float32, name='B')\n",
    "\n",
    "Q = tf.constant([[1.,0.], [0.,1.]], name='Q')  # weighting\n",
    "R = tf.constant(1., name='R')\n",
    "\n",
    "x = tf.constant(x0, dtype=tf.float32, name=\"state\")\n",
    "\n",
    "T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### Defining network #######\n",
    "# input: state x\n",
    "# output: control u\n",
    "\n",
    "input_layer = tf.placeholder(tf.float32, (None,2), name='in_layer')\n",
    "# fc1 = tf.layers.dense(inputs=input_layer, units=2,) #activation=tf.nn.sigmoid)\n",
    "# fc2 = tf.layers.dense(inputs=fc1, units=2, activation=tf.nn.sigmoid)\n",
    "u = tf.layers.dense(inputs=input_layer, units=1, name='u_out_layer', reuse=tf.AUTO_REUSE)\n",
    "\n",
    "### LOSS FUNCTION ### \n",
    "loss = tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), \n",
    "              tf.matmul(tf.transpose(u), tf.multiply(R, u)), name='loss')\n",
    "\n",
    "# xs = tf.identity(x, name='xs')\n",
    "# us = tf.constant(0, name='us')\n",
    "xs = x\n",
    "us = u\n",
    "\n",
    "# cond = lambda i, x, l, xs, us: i < T\n",
    "\n",
    "# def body(i, x, l, xs, us):\n",
    "#     next_i = i+1\n",
    "#     next_x = tf.add(tf.matmul(A, x), tf.multiply(u,B))\n",
    "#     next_l = tf.add(l,\n",
    "#                     tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x),\n",
    "#                            tf.matmul(tf.transpose(u), tf.multiply(R, u))))\n",
    "#     next_xs = tf.concat(xs, next_x)\n",
    "#     next_us = tf.concat(us, u)\n",
    "#     return (next_i, next_x, next_l, next_xs, next_us)\n",
    "\n",
    "# i, xloss_f, traj_f = tf.while_loop(cond, body, \n",
    "#                                    loop_vars=[tf.constant(0), x, loss, xs, us],\n",
    "#                                    shape_invariants=[tf.TensorShape([1,]), tf.TensorShape([2, 1]), \n",
    "#                                                      tf.TensorShape([1,]) , tf.TensorShape([2, None]), \n",
    "#                                                      tf.TensorShape([1, None])])\n",
    "# train = tf.train.GradientDescentOptimizer(0.01).minimize(xloss_f.loss)\n",
    "\n",
    "for i in range(T):\n",
    "    # LQR loss \n",
    "#     x_term = tf.matmul(tf.matmul(tf.transpose(x), Q), x, name='x_term')\n",
    "#     u_term = tf.matmul(tf.transpose(u), tf.multiply(R, u), name='u_term')\n",
    "#     loss = tf.add(loss, tf.add(x_term, u_term), name='loss')  # accumulate loss\n",
    "    \n",
    "    # Dynamics: advancing the system dynamics\n",
    "    Ax = tf.matmul(A, x, name='Ax'+str(i))\n",
    "    Bu = tf.multiply(u, B, name='Bu'+str(i))  # tf.multiply because u is a scalar\n",
    "    x = tf.add(Ax, Bu, name='state'+str(i))  # next state vector\n",
    "\n",
    "    loss = tf.add(loss, tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), tf.matmul(tf.transpose(u), tf.multiply(R, u))), name='loss'+str(i))  # accumulate loss    \n",
    "    \n",
    "    u = tf.layers.dense(inputs=tf.transpose(x), units=1, name='u_out_layer', reuse=True)\n",
    "    \n",
    "    xs = tf.concat([xs, x], 1)\n",
    "    us = tf.concat([us, u], 1)\n",
    "    \n",
    "opt = tf.train.GradientDescentOptimizer(0.0001)\n",
    "train = opt.minimize(loss)\n",
    "grads_and_vars = opt.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training iteration 0\n",
      "xs\n",
      "[[ 1.         -0.7173682   0.49605566 -0.2926046   0.1731691  -0.06723218\n",
      "   0.02857424  0.01676741 -0.01364223  0.03252974 -0.01430157]\n",
      " [ 1.         -0.8173682   0.62779254 -0.4212523   0.27961823 -0.14841858\n",
      "   0.0840093  -0.01935105  0.00635211  0.02189736 -0.01117512]]\n",
      "\n",
      "us\n",
      "[[-0.21736817  0.13737158 -0.04457678  0.02686681  0.01935238 -0.00504185\n",
      "   0.03105453 -0.00525853  0.02570863  0.0019633   0.0186675 ]]\n",
      "\n",
      "loss\n",
      "[[4.349453]]\n",
      "\n",
      "gradients\n",
      "[[-12.06689]\n",
      " [-13.08315]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-4.0653143] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 50\n",
      "xs\n",
      "[[ 1.00000000e+00 -6.09071851e-01  3.70836347e-01 -1.49988860e-01\n",
      "   7.31519759e-02  2.10332200e-02 -1.04900915e-02  5.09902090e-02\n",
      "  -1.15798619e-02  3.94421443e-02  3.19138542e-03]\n",
      " [ 1.00000000e+00 -7.09071875e-01  4.91743565e-01 -2.59616822e-01\n",
      "   1.53927639e-01 -3.47473770e-02  2.08749454e-02  3.32201943e-02\n",
      "  -6.01687469e-03  3.72623391e-02  5.55053353e-04]]\n",
      "\n",
      "us\n",
      "[[-1.0907187e-01  6.6300429e-02  3.5429321e-02 -1.8424559e-03\n",
      "   5.7609208e-02  2.6518479e-05  4.5745164e-02  1.3915243e-02\n",
      "   3.3652212e-02  2.2912458e-02  2.7715852e-02]]\n",
      "\n",
      "loss\n",
      "[[3.4177597]]\n",
      "\n",
      "gradients\n",
      "[[-6.8594813]\n",
      " [-7.2658134]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-2.282176] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 100\n",
      "xs\n",
      "[[ 1.0000000e+00 -5.4090911e-01  3.0101886e-01 -7.8480691e-02\n",
      "   3.3930570e-02  5.2416608e-02 -1.4119592e-02  5.6979612e-02\n",
      "  -3.7669577e-04  3.8958598e-02  1.5403351e-02]\n",
      " [ 1.0000000e+00 -6.4090914e-01  4.1510981e-01 -1.7703715e-01\n",
      "   1.0091252e-01  8.8343807e-03  6.7880838e-03  4.5846961e-02\n",
      "   6.0493127e-04  3.8407292e-02  1.1838274e-02]]\n",
      "\n",
      "us\n",
      "[[-0.04090914  0.03056431  0.07202874 -0.00530978  0.06938189  0.01208871\n",
      "   0.04991981  0.02811311  0.03877025  0.03488265  0.03522532]]\n",
      "\n",
      "loss\n",
      "[[3.047362]]\n",
      "\n",
      "gradients\n",
      "[[-4.8900995]\n",
      " [-5.1155577]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-1.309378] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 150\n",
      "xs\n",
      "[[ 1.         -0.49200347  0.25369066 -0.03502794  0.01394852  0.06628399\n",
      "  -0.01143296  0.05722249  0.00834141  0.03816317  0.02258427]\n",
      " [ 1.         -0.5920035   0.36289108 -0.12591726  0.0719849   0.0300673\n",
      "   0.00366865  0.04930481  0.00736976  0.03791202  0.01891864]]\n",
      "\n",
      "us\n",
      "[[ 0.00799652  0.00768893  0.09181739 -0.00356545  0.07325824  0.02170903\n",
      "   0.051506    0.03695265  0.04233388  0.04166585  0.04041251]]\n",
      "\n",
      "loss\n",
      "[[2.8480902]]\n",
      "\n",
      "gradients\n",
      "[[-3.7993617]\n",
      " [-3.9406252]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-0.67763835] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 200\n",
      "xs\n",
      "[[ 1.         -0.45484397  0.21843082 -0.00635774  0.00217875  0.07251132\n",
      "  -0.00789987  0.05562817  0.0142504   0.03724515  0.0266204 ]\n",
      " [ 1.         -0.554844    0.32391527 -0.09149151  0.05389478  0.04126382\n",
      "   0.00359749  0.04951974  0.01235264  0.03695877  0.02306772]]\n",
      "\n",
      "us\n",
      "[[ 0.04515602 -0.00899116  0.10285766 -0.00100012  0.07360069  0.02835578\n",
      "   0.05167823  0.04206448  0.04437035  0.04524298  0.04346506]]\n",
      "\n",
      "loss\n",
      "[[2.7250373]]\n",
      "\n",
      "gradients\n",
      "[[-3.0936885]\n",
      " [-3.1872396]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-0.24843675] - u_out_layer/bias:0\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# show_graph(tf.get_default_graph().as_graph_def())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(250):\n",
    "#         sess.run(train)\n",
    "        sess.run(train, feed_dict={input_layer : x0.T})\n",
    "        if i % 50 == 0:\n",
    "#             results = sess.run([xs, us, loss, grads_and_vars], feed_dict={input_layer : x0.T})\n",
    "            results = sess.run([xs, us, loss], feed_dict={input_layer : x0.T})\n",
    "            labels  = \"xs us loss\".split(' ')\n",
    "            print('training iteration', i)\n",
    "            for label,result in zip(*(labels,results)) :\n",
    "                print(label)\n",
    "                print(result)\n",
    "                print('')\n",
    "            for g, v in grads_and_vars:\n",
    "                print('gradients')\n",
    "                print(str(sess.run(g, feed_dict={input_layer : x0.T})) + \" - \" + v.name)\n",
    "            print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## compute optimal loss with true LQR ricatti equation formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import controlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11926123]\n",
      " [-0.21926125]] [[2.14496201]] \n",
      "-------------------------------\n",
      "[[-0.00746017]\n",
      " [ 0.06446597]] [[2.21176192]] \n",
      "-------------------------------\n",
      "[[ 0.01648575]\n",
      " [-0.02592392]] [[2.21613615]] \n",
      "-------------------------------\n",
      "[[-0.01116197]\n",
      " [ 0.01263526]] [[2.2170885]] \n",
      "-------------------------------\n",
      "[[ 0.00649124]\n",
      " [-0.0066709 ]] [[2.21737356]] \n",
      "-------------------------------\n",
      "[[-0.00362815]\n",
      " [ 0.00362001]] [[2.21746035]] \n",
      "-------------------------------\n",
      "[[ 0.00200435]\n",
      " [-0.00198173]] [[2.21748665]] \n",
      "-------------------------------\n",
      "[[-0.00110336]\n",
      " [ 0.00108785]] [[2.21749461]] \n",
      "-------------------------------\n",
      "[[ 0.00060672]\n",
      " [-0.00059767]] [[2.21749701]] \n",
      "-------------------------------\n",
      "[[-0.00033351]\n",
      " [ 0.00032845]] [[2.21749774]] \n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "if type(A) != np.ndarray:  # if tensors not already evaluated\n",
    "    with tf.Session() as sess:\n",
    "        A, B, Q, R = sess.run([A, B, Q, R])\n",
    "\n",
    "K, P, eig = controlpy.synthesis.controller_lqr_discrete_time(A, B, Q, R)\n",
    "x = x0\n",
    "loss = 0\n",
    "for i in range(T):\n",
    "    u = (-K@x)\n",
    "    loss += x.T@Q@x + u.T*R*u\n",
    "    x = A@x + B@u\n",
    "    print(x, loss, '\\n-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
