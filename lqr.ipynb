{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/flow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/envs/flow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ray.experimental.tfutils import TensorFlowVariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1600px;height:750px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.001, 0], [0, 0.5]])\n",
    "B = np.array([[1.], [1.]])\n",
    "\n",
    "# for controllability\n",
    "cont = np.hstack([B, A@B, A@A@B])\n",
    "assert np.linalg.matrix_rank(cont)==cont.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[1.], [1.]])  # 2x1 vector \n",
    "\n",
    "A = tf.constant(np.array([[1.001, 0], [0, -0.6]]), dtype=tf.float32, name='A')  # system dynamics\n",
    "B = tf.constant(np.array([[1.], [1.]]), dtype=tf.float32, name='B')\n",
    "\n",
    "Q = tf.constant([[1.,0.], [0.,1.]], name='Q')  # weighting\n",
    "R = tf.constant(1., name='R')\n",
    "\n",
    "x = tf.constant(x0, dtype=tf.float32, name=\"state\")\n",
    "\n",
    "T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### Defining network #######\n",
    "# input: state x\n",
    "# output: control u\n",
    "\n",
    "input_layer = tf.placeholder(tf.float32, (None,2), name='in_layer')\n",
    "# fc1 = tf.layers.dense(inputs=input_layer, units=2,) #activation=tf.nn.sigmoid)\n",
    "# fc2 = tf.layers.dense(inputs=fc1, units=2, activation=tf.nn.sigmoid)\n",
    "u = tf.layers.dense(inputs=input_layer, units=1, name='u_out_layer', reuse=tf.AUTO_REUSE)\n",
    "\n",
    "### LOSS FUNCTION ### \n",
    "loss = tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), \n",
    "              tf.matmul(tf.transpose(u), tf.multiply(R, u)), name='loss')\n",
    "\n",
    "# xs = tf.identity(x, name='xs')\n",
    "# us = tf.constant(0, name='us')\n",
    "xs = x\n",
    "us = u\n",
    "\n",
    "# cond = lambda i, x, l, xs, us: i < T\n",
    "\n",
    "# def body(i, x, l, xs, us):\n",
    "#     next_i = i+1\n",
    "#     next_x = tf.add(tf.matmul(A, x), tf.multiply(u,B))\n",
    "#     next_l = tf.add(l,\n",
    "#                     tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x),\n",
    "#                            tf.matmul(tf.transpose(u), tf.multiply(R, u))))\n",
    "#     next_xs = tf.concat(xs, next_x)\n",
    "#     next_us = tf.concat(us, u)\n",
    "#     return (next_i, next_x, next_l, next_xs, next_us)\n",
    "\n",
    "# i, xloss_f, traj_f = tf.while_loop(cond, body, \n",
    "#                                    loop_vars=[tf.constant(0), x, loss, xs, us],\n",
    "#                                    shape_invariants=[tf.TensorShape([1,]), tf.TensorShape([2, 1]), \n",
    "#                                                      tf.TensorShape([1,]) , tf.TensorShape([2, None]), \n",
    "#                                                      tf.TensorShape([1, None])])\n",
    "# train = tf.train.GradientDescentOptimizer(0.01).minimize(xloss_f.loss)\n",
    "\n",
    "for i in range(T):\n",
    "    # LQR loss \n",
    "#     x_term = tf.matmul(tf.matmul(tf.transpose(x), Q), x, name='x_term')\n",
    "#     u_term = tf.matmul(tf.transpose(u), tf.multiply(R, u), name='u_term')\n",
    "#     loss = tf.add(loss, tf.add(x_term, u_term), name='loss')  # accumulate loss\n",
    "    \n",
    "    # Dynamics: advancing the system dynamics\n",
    "    Ax = tf.matmul(A, x, name='Ax'+str(i))\n",
    "    Bu = tf.multiply(u, B, name='Bu'+str(i))  # tf.multiply because u is a scalar\n",
    "    x = tf.add(Ax, Bu, name='state'+str(i))  # next state vector\n",
    "\n",
    "    loss = tf.add(loss, tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), tf.matmul(tf.transpose(u), tf.multiply(R, u))), name='loss'+str(i))  # accumulate loss    \n",
    "    \n",
    "    u = tf.layers.dense(inputs=tf.transpose(x), units=1, name='u_out_layer', reuse=True)\n",
    "    \n",
    "    xs = tf.concat([xs, x], 1)\n",
    "    us = tf.concat([us, u], 1)\n",
    "    \n",
    "opt = tf.train.GradientDescentOptimizer(0.0001)\n",
    "train = opt.minimize(loss)\n",
    "grads_and_vars = opt.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training iteration 0\n",
      "xs\n",
      "[[ 1.          1.0583686  -0.71114707 -1.573868    0.16657186  1.9830533\n",
      "   0.7047497  -2.0511045  -1.7490053   1.6986771   2.879787  ]\n",
      " [ 1.         -0.5426315  -1.4449952   0.00498736  1.7390214   0.772902\n",
      "  -1.744028   -1.710142    1.3302356   2.65129    -0.41136277]]\n",
      "\n",
      "us\n",
      "[[ 0.05736851 -1.7705741  -0.86200976  1.7420138   1.8163149  -1.2802868\n",
      "  -2.756559    0.30415034  3.4494314   1.1794113  -3.6177697 ]]\n",
      "\n",
      "loss\n",
      "[[82.78097]]\n",
      "\n",
      "gradients\n",
      "[[-201.90422]\n",
      " [ 468.51456]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-61.22252] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 1\n",
      "xs\n",
      "[[ 1.          1.0378298  -0.6792079  -1.4457271   0.1938355   1.7605584\n",
      "   0.54662085 -1.7553871  -1.3376389   1.4693614   2.1743221 ]\n",
      " [ 1.         -0.5631703  -1.3801733   0.06226414  1.6036499   0.60433906\n",
      "  -1.5783015  -1.3555737   1.2328479   2.0686293  -0.5376862 ]]\n",
      "\n",
      "us\n",
      "[[ 0.03682972 -1.7180755  -0.7658399   1.6410084   1.566529   -1.2156981\n",
      "  -2.3025546   0.4195037   2.808338    0.70349133 -2.9134161 ]]\n",
      "\n",
      "loss\n",
      "[[60.42679]]\n",
      "\n",
      "gradients\n",
      "[[-154.488  ]\n",
      " [ 327.36096]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-46.77838] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 2\n",
      "xs\n",
      "[[ 1.          1.0252204  -0.6527201  -1.3582914   0.20391977  1.6117332\n",
      "   0.46059906 -1.5542504  -1.1008799   1.3003868   1.7712708 ]\n",
      " [ 1.         -0.57577974 -1.333498    0.09518033  1.5064614   0.5037326\n",
      "  -1.4549856  -1.1423187   1.1403161   1.7181779  -0.56132334]]\n",
      "\n",
      "us\n",
      "[[ 0.0242203  -1.6789658  -0.7049185   1.5635695   1.4076095  -1.152746\n",
      "  -2.01531     0.45492488  2.4023676   0.46958345 -2.455087  ]]\n",
      "\n",
      "loss\n",
      "[[48.593174]]\n",
      "\n",
      "gradients\n",
      "[[-126.6662 ]\n",
      " [ 254.24788]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-38.627495] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 3\n",
      "xs\n",
      "[[ 1.          1.0163249  -0.63004935 -1.2915318   0.20776415  1.5006355\n",
      "   0.40539038 -1.403641   -0.94177544  1.1706579   1.5038289 ]\n",
      " [ 1.         -0.58467525 -1.2965854   0.11709893  1.4303282   0.4344666\n",
      "  -1.3574258  -0.9949813   1.060258    1.4772203  -0.554332  ]]\n",
      "\n",
      "us\n",
      "[[ 0.0153248  -1.6473906  -0.6608524   1.5005876   1.2926636  -1.0967458\n",
      "  -1.8094368   0.4632692   2.1133752   0.33200023 -2.1269093 ]]\n",
      "\n",
      "loss\n",
      "[[41.105873]]\n",
      "\n",
      "gradients\n",
      "[[-107.82558]\n",
      " [ 208.74292]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-33.233772] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 4\n",
      "xs\n",
      "[[ 1.          1.0095565  -0.61020315 -1.237439    0.20883954  1.4126414\n",
      "   0.36659563 -1.2845794  -0.8254601   1.0675166   1.3112305 ]\n",
      " [ 1.         -0.59144354 -1.2659031   0.13291627  1.3677664   0.38293314\n",
      "  -1.2772185  -0.88521045  0.9915302   1.298884   -0.5366839 ]]\n",
      "\n",
      "us\n",
      "[[ 0.00855651 -1.6207693  -0.62662566  1.4475161   1.203593   -1.0474585\n",
      "  -1.6515416   0.4604039   1.8938022   0.24264649 -1.8780259 ]]\n",
      "\n",
      "loss\n",
      "[[35.893097]]\n",
      "\n",
      "gradients\n",
      "[[-94.069244]\n",
      " [177.46777 ]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-29.347975] - u_out_layer/bias:0\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# show_graph(tf.get_default_graph().as_graph_def())\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# variables = TensorFlowVariables(loss, sess)\n",
    "# sess.close()\n",
    "    \n",
    "    \n",
    "with tf.Session() as sess:    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5):\n",
    "#         sess.run(train)\n",
    "        sess.run(train, feed_dict={input_layer : x0.T})\n",
    "        if i % 1 == 0:\n",
    "#             results = sess.run([xs, us, loss, grads_and_vars], feed_dict={input_layer : x0.T})\n",
    "            results = sess.run([xs, us, loss], feed_dict={input_layer : x0.T})\n",
    "            labels  = \"xs us loss\".split(' ')\n",
    "            print('training iteration', i)\n",
    "            for label,result in zip(*(labels,results)) :\n",
    "                print(label)\n",
    "                print(result)\n",
    "                print('')\n",
    "            for g, v in grads_and_vars:\n",
    "                print('gradients')\n",
    "                print(str(sess.run(g, feed_dict={input_layer : x0.T})) + \" - \" + v.name)\n",
    "            print('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## compute optimal loss with true LQR ricatti equation formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import controlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if type(A) != np.ndarray:  # if tensors not already evaluated\n",
    "    with tf.Session() as sess:\n",
    "        A, B, Q, R = sess.run([A, B, Q, R])\n",
    "\n",
    "K, P, eig = controlpy.synthesis.controller_lqr_discrete_time(A, B, Q, R)\n",
    "x = x0\n",
    "u = (-K@x)\n",
    "xs, us = np.array(x), np.array(u)\n",
    "loss = 0\n",
    "for i in range(T):\n",
    "    loss += x.T@Q@x + u.T*R*u\n",
    "    x = A@x + B@u\n",
    "    u = (-K@x)\n",
    "    xs = np.hstack([xs, x])\n",
    "    us = np.hstack([us, u])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
