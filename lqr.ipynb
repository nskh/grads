{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/flow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1600px;height:750px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.001, 0], [0, 0.5]])\n",
    "B = np.array([[1.], [1.]])\n",
    "\n",
    "# for controllability\n",
    "cont = np.hstack([B, A@B, A@A@B])\n",
    "assert np.linalg.matrix_rank(cont)==cont.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[1.], [1.]])  # 2x1 vector \n",
    "\n",
    "A = tf.constant(np.array([[1.001, 0], [0, -0.6]]), dtype=tf.float32, name='A')  # system dynamics\n",
    "B = tf.constant(np.array([[1.], [1.]]), dtype=tf.float32, name='B')\n",
    "\n",
    "Q = tf.constant([[1.,0.], [0.,1.]], name='Q')  # weighting\n",
    "R = tf.constant(1., name='R')\n",
    "\n",
    "x = tf.constant(x0, dtype=tf.float32, name=\"state\")\n",
    "\n",
    "T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### Defining network #######\n",
    "# input: state x\n",
    "# output: control u\n",
    "\n",
    "input_layer = tf.placeholder(tf.float32, (None,2), name='in_layer')\n",
    "# fc1 = tf.layers.dense(inputs=input_layer, units=2,) #activation=tf.nn.sigmoid)\n",
    "# fc2 = tf.layers.dense(inputs=fc1, units=2, activation=tf.nn.sigmoid)\n",
    "u = tf.layers.dense(inputs=input_layer, units=1, name='u_out_layer', reuse=tf.AUTO_REUSE)\n",
    "\n",
    "### LOSS FUNCTION ### \n",
    "loss = tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), \n",
    "              tf.matmul(tf.transpose(u), tf.multiply(R, u)), name='loss')\n",
    "\n",
    "# xs = tf.identity(x, name='xs')\n",
    "# us = tf.constant(0, name='us')\n",
    "xs = x\n",
    "us = u\n",
    "\n",
    "# cond = lambda i, x, l, xs, us: i < T\n",
    "\n",
    "# def body(i, x, l, xs, us):\n",
    "#     next_i = i+1\n",
    "#     next_x = tf.add(tf.matmul(A, x), tf.multiply(u,B))\n",
    "#     next_l = tf.add(l,\n",
    "#                     tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x),\n",
    "#                            tf.matmul(tf.transpose(u), tf.multiply(R, u))))\n",
    "#     next_xs = tf.concat(xs, next_x)\n",
    "#     next_us = tf.concat(us, u)\n",
    "#     return (next_i, next_x, next_l, next_xs, next_us)\n",
    "\n",
    "# i, xloss_f, traj_f = tf.while_loop(cond, body, \n",
    "#                                    loop_vars=[tf.constant(0), x, loss, xs, us],\n",
    "#                                    shape_invariants=[tf.TensorShape([1,]), tf.TensorShape([2, 1]), \n",
    "#                                                      tf.TensorShape([1,]) , tf.TensorShape([2, None]), \n",
    "#                                                      tf.TensorShape([1, None])])\n",
    "# train = tf.train.GradientDescentOptimizer(0.01).minimize(xloss_f.loss)\n",
    "\n",
    "for i in range(T):\n",
    "    # LQR loss \n",
    "#     x_term = tf.matmul(tf.matmul(tf.transpose(x), Q), x, name='x_term')\n",
    "#     u_term = tf.matmul(tf.transpose(u), tf.multiply(R, u), name='u_term')\n",
    "#     loss = tf.add(loss, tf.add(x_term, u_term), name='loss')  # accumulate loss\n",
    "    \n",
    "    # Dynamics: advancing the system dynamics\n",
    "    Ax = tf.matmul(A, x, name='Ax'+str(i))\n",
    "    Bu = tf.multiply(u, B, name='Bu'+str(i))  # tf.multiply because u is a scalar\n",
    "    x = tf.add(Ax, Bu, name='state'+str(i))  # next state vector\n",
    "\n",
    "    loss = tf.add(loss, tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), tf.matmul(tf.transpose(u), tf.multiply(R, u))), name='loss'+str(i))  # accumulate loss    \n",
    "    \n",
    "    u = tf.layers.dense(inputs=tf.transpose(x), units=1, name='u_out_layer', reuse=True)\n",
    "    \n",
    "    xs = tf.concat([xs, x], 1)\n",
    "    us = tf.concat([us, u], 1)\n",
    "    \n",
    "opt = tf.train.GradientDescentOptimizer(0.0001)\n",
    "train = opt.minimize(loss)\n",
    "grads_and_vars = opt.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training iteration 0\n",
      "xs\n",
      "[[ 1.00000000e+00  3.52078438e-01 -4.16622281e-01  2.90835500e-02\n",
      "   1.85335875e-01 -1.90454572e-02 -1.95856355e-02  5.46228886e-02\n",
      "   3.44566405e-02  1.30995903e-02  2.62672342e-02]\n",
      " [ 1.00000000e+00 -1.24892163e+00 -1.96998119e-02  4.57942367e-01\n",
      "  -1.18542194e-01 -1.33441359e-01  7.95436874e-02  2.65018940e-02\n",
      "  -3.61220092e-02  2.81697139e-04  1.29855257e-02]]\n",
      "\n",
      "us\n",
      "[[-6.4892161e-01 -7.6905280e-01  4.4612247e-01  1.5622324e-01\n",
      "  -2.0456667e-01 -5.2113272e-04  7.4228108e-02 -2.0220870e-02\n",
      "  -2.1391509e-02  1.3154544e-02  4.0996876e-03]]\n",
      "\n",
      "loss\n",
      "[[5.8542423]]\n",
      "\n",
      "gradients\n",
      "[[-7.328549]\n",
      " [ 2.542591]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-6.248411] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 50\n",
      "xs\n",
      "[[ 1.00000000e+00  4.03491318e-01 -3.44522417e-01  4.06522453e-02\n",
      "   1.91251963e-01  2.60444731e-02  1.77453067e-02  7.44630247e-02\n",
      "   6.29590303e-02  4.70929295e-02  5.48830442e-02]\n",
      " [ 1.00000000e+00 -1.19750881e+00 -2.99119353e-02  4.03466374e-01\n",
      "  -9.15207714e-02 -1.10486284e-01  5.79665601e-02  2.19200328e-02\n",
      "  -2.47304831e-02 -1.09077524e-03  8.39748513e-03]]\n",
      "\n",
      "us\n",
      "[[-0.5975087  -0.74841726  0.3855192   0.15055907 -0.16539875 -0.00832521\n",
      "   0.05669997 -0.01157846 -0.01592907  0.00774302  0.0032349 ]]\n",
      "\n",
      "loss\n",
      "[[5.4348207]]\n",
      "\n",
      "gradients\n",
      "[[-6.395776 ]\n",
      " [ 1.9312538]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-5.053417] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 100\n",
      "xs\n",
      "[[ 1.          0.44760555 -0.28306615  0.05276862  0.19774655  0.06233072\n",
      "   0.04936872  0.09351733  0.08731382  0.07531682  0.0799595 ]\n",
      " [ 1.         -1.1533945  -0.03908265  0.35956746 -0.07081534 -0.09312437\n",
      "   0.04285029  0.01838907 -0.01733047 -0.00168604  0.00557898]]\n",
      "\n",
      "us\n",
      "[[-0.5533945  -0.73111933  0.33611786  0.14492515 -0.13561358 -0.01302434\n",
      "   0.04409924 -0.00629703 -0.01208432  0.00456735  0.00250068]]\n",
      "\n",
      "loss\n",
      "[[5.1361704]]\n",
      "\n",
      "gradients\n",
      "[[-5.636971 ]\n",
      " [ 1.5353092]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-4.0204926] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 150\n",
      "xs\n",
      "[[ 1.          0.4850821  -0.23064485  0.06451567  0.20366901  0.09145632\n",
      "   0.07592449  0.11073093  0.10778172  0.09861769  0.10137264]\n",
      " [ 1.         -1.1159179  -0.04666126  0.32338792 -0.05494393 -0.07945001\n",
      "   0.03204671  0.01550249 -0.01236144 -0.00185496  0.00376931]]\n",
      "\n",
      "us\n",
      "[[-0.51591796 -0.71621203  0.29539117  0.13908882 -0.11241637 -0.0156233\n",
      "   0.03473051 -0.00305995 -0.00927182  0.00265633  0.00189558]]\n",
      "\n",
      "loss\n",
      "[[4.9210467]]\n",
      "\n",
      "gradients\n",
      "[[-5.0035124]\n",
      " [ 1.2658589]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-3.1301758] - u_out_layer/bias:0\n",
      "----------------------\n",
      "training iteration 200\n",
      "xs\n",
      "[[ 1.          0.51665145 -0.18604863  0.07527968  0.20845884  0.11462468\n",
      "   0.09790526  0.12561634  0.1246513   0.11760291  0.11921045]\n",
      " [ 1.         -1.0843487  -0.05260748  0.29307884 -0.04274343 -0.06839657\n",
      "   0.02420389  0.01309085 -0.00894517 -0.00180594  0.00257349]]\n",
      "\n",
      "us\n",
      "[[-0.4843486  -0.70321673  0.26151437  0.13310388 -0.09404263 -0.01683405\n",
      "   0.02761319 -0.00109066 -0.00717304  0.00148993  0.00141159]]\n",
      "\n",
      "loss\n",
      "[[4.7643266]]\n",
      "\n",
      "gradients\n",
      "[[-4.4686737]\n",
      " [ 1.0742294]] - u_out_layer/kernel:0\n",
      "gradients\n",
      "[-2.3684604] - u_out_layer/bias:0\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# show_graph(tf.get_default_graph().as_graph_def())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(250):\n",
    "#         sess.run(train)\n",
    "        sess.run(train, feed_dict={input_layer : x0.T})\n",
    "        if i % 50 == 0:\n",
    "#             results = sess.run([xs, us, loss, grads_and_vars], feed_dict={input_layer : x0.T})\n",
    "            results = sess.run([xs, us, loss], feed_dict={input_layer : x0.T})\n",
    "            labels  = \"xs us loss\".split(' ')\n",
    "            print('training iteration', i)\n",
    "            for label,result in zip(*(labels,results)) :\n",
    "                print(label)\n",
    "                print(result)\n",
    "                print('')\n",
    "            for g, v in grads_and_vars:\n",
    "                print('gradients')\n",
    "                print(str(sess.run(g, feed_dict={input_layer : x0.T})) + \" - \" + v.name)\n",
    "            print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## compute optimal loss with true LQR ricatti equation formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import controlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if type(A) != np.ndarray:  # if tensors not already evaluated\n",
    "    with tf.Session() as sess:\n",
    "        A, B, Q, R = sess.run([A, B, Q, R])\n",
    "\n",
    "K, P, eig = controlpy.synthesis.controller_lqr_discrete_time(A, B, Q, R)\n",
    "x = x0\n",
    "u = (-K@x)\n",
    "xs, us = np.array(x), np.array(u)\n",
    "loss = 0\n",
    "for i in range(T):\n",
    "    loss += x.T@Q@x + u.T*R*u\n",
    "    x = A@x + B@u\n",
    "    u = (-K@x)\n",
    "    xs = np.hstack([xs, x])\n",
    "    us = np.hstack([us, u])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
