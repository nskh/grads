{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/flow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/envs/flow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ray.experimental.tfutils import TensorFlowVariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1600px;height:750px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "    \n",
    "def eval_grads(new_weights):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables = TensorFlowVariables(loss, sess)\n",
    "    variables.set_weights(new_weights)\n",
    "    print('gradients')\n",
    "    for g, v in grads_and_vars:\n",
    "        print(str(sess.run(g, feed_dict={input_layer : x0.T})) + \" - \" + v.name)\n",
    "    sess.close()\n",
    "    \n",
    "def process_weights(w):\n",
    "    nw = dict()\n",
    "    nw['fc1/kernel'] = w['fc1/weights']\n",
    "    nw['fc1/bias'] = w['fc1/biases']\n",
    "    nw['fc_out/kernel'] = w['fc_out/weights']\n",
    "    nw['fc_out/bias'] = w['fc_out/biases']\n",
    "    return nw        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.001, 0], [0, 0.5]])\n",
    "B = np.array([[1.], [1.]])\n",
    "\n",
    "# for controllability\n",
    "cont = np.hstack([B, A@B, A@A@B])\n",
    "assert np.linalg.matrix_rank(cont)==cont.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[1.], [1.]])  # 2x1 vector \n",
    "\n",
    "A = tf.constant(np.array([[1.001, 0], [0, -0.6]]), dtype=tf.float32, name='A')  # system dynamics\n",
    "B = tf.constant(np.array([[1.], [1.]]), dtype=tf.float32, name='B')\n",
    "\n",
    "Q = tf.constant([[1.,0.], [0.,1.]], name='Q')  # weighting\n",
    "R = tf.constant(1., name='R')\n",
    "\n",
    "x = tf.constant(x0, dtype=tf.float32, name=\"state\")\n",
    "\n",
    "T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### Defining network #######\n",
    "# input: state x\n",
    "# output: control u\n",
    "\n",
    "input_layer = tf.placeholder(tf.float32, (None,2), name='in_layer')\n",
    "fc1 = tf.layers.dense(inputs=input_layer, units=1, activation=tf.nn.tanh, name='fc1', reuse=tf.AUTO_REUSE)\n",
    "u = tf.layers.dense(inputs=fc1, units=1, activation=tf.nn.tanh, name='fc_out', reuse=tf.AUTO_REUSE)\n",
    "# u = tf.layers.dense(inputs=input_layer, units=1, name='u_out_layer', reuse=tf.AUTO_REUSE)\n",
    "\n",
    "### LOSS FUNCTION ### \n",
    "loss = tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), \n",
    "              tf.matmul(tf.transpose(u), tf.multiply(R, u)), name='loss')\n",
    "\n",
    "# xs = tf.identity(x, name='xs')\n",
    "# us = tf.constant(0, name='us')\n",
    "xs = x\n",
    "us = u\n",
    "\n",
    "# cond = lambda i, x, l, xs, us: i < T\n",
    "\n",
    "# def body(i, x, l, xs, us):\n",
    "#     next_i = i+1\n",
    "#     next_x = tf.add(tf.matmul(A, x), tf.multiply(u,B))\n",
    "#     next_l = tf.add(l,\n",
    "#                     tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x),\n",
    "#                            tf.matmul(tf.transpose(u), tf.multiply(R, u))))\n",
    "#     next_xs = tf.concat(xs, next_x)\n",
    "#     next_us = tf.concat(us, u)\n",
    "#     return (next_i, next_x, next_l, next_xs, next_us)\n",
    "\n",
    "# i, xloss_f, traj_f = tf.while_loop(cond, body, \n",
    "#                                    loop_vars=[tf.constant(0), x, loss, xs, us],\n",
    "#                                    shape_invariants=[tf.TensorShape([1,]), tf.TensorShape([2, 1]), \n",
    "#                                                      tf.TensorShape([1,]) , tf.TensorShape([2, None]), \n",
    "#                                                      tf.TensorShape([1, None])])\n",
    "# train = tf.train.GradientDescentOptimizer(0.01).minimize(xloss_f.loss)\n",
    "\n",
    "for i in range(T):\n",
    "    # LQR loss \n",
    "#     x_term = tf.matmul(tf.matmul(tf.transpose(x), Q), x, name='x_term')\n",
    "#     u_term = tf.matmul(tf.transpose(u), tf.multiply(R, u), name='u_term')\n",
    "#     loss = tf.add(loss, tf.add(x_term, u_term), name='loss')  # accumulate loss\n",
    "    \n",
    "    # Dynamics: advancing the system dynamics\n",
    "    Ax = tf.matmul(A, x, name='Ax'+str(i))\n",
    "    Bu = tf.multiply(u, B, name='Bu'+str(i))  # tf.multiply because u is a scalar\n",
    "    x = tf.add(Ax, Bu, name='state'+str(i))  # next state vector\n",
    "\n",
    "    loss = tf.add(loss, tf.add(tf.matmul(tf.matmul(tf.transpose(x), Q), x), tf.matmul(tf.transpose(u), tf.multiply(R, u))), name='loss'+str(i))  # accumulate loss    \n",
    "    \n",
    "#     u = tf.layers.dense(inputs=tf.transpose(x), units=1, name='u_out_layer', reuse=True)\n",
    "    \n",
    "    fc1 = tf.layers.dense(inputs=tf.transpose(x), units=1, name='fc1', reuse=True)\n",
    "    u = tf.layers.dense(inputs=fc1, units=1, name='fc_out', reuse=True)\n",
    "    \n",
    "    xs = tf.concat([xs, x], 1)\n",
    "    us = tf.concat([us, u], 1)\n",
    "    \n",
    "opt = tf.train.GradientDescentOptimizer(0.0001)\n",
    "train = opt.minimize(loss)\n",
    "grads_and_vars = opt.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_weights = v[-1]['weights'].copy()\n",
    "# new_weights['u_out_layer/bias'] = new_weights.pop('fc1/biases')\n",
    "# new_weights['u_out_layer/kernel'] = new_weights.pop('fc1/weights')\n",
    "# del new_weights['fc_out/biases']\n",
    "# del new_weights['fc_out/weights']\n",
    "# new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training iteration 0\n",
      "xs\n",
      "[[ 1.          1.0531694   1.0574822   1.1169957   1.1529142   1.2085396\n",
      "   1.2611325   1.3237815   1.390256    1.4648727   1.5463333 ]\n",
      " [ 1.         -0.54783064  0.331958   -0.14071882  0.11923278 -0.01706719\n",
      "   0.06162465  0.02441302  0.05050296  0.04292452  0.05424096]]\n",
      "\n",
      "us\n",
      "[[0.05216937 0.00325961 0.058456   0.03480149 0.05447248 0.05138434\n",
      "  0.06138781 0.06515077 0.07322629 0.07999568 0.0886057 ]]\n",
      "\n",
      "loss\n",
      "[[18.569374]]\n",
      "\n",
      "gradients\n",
      "[[-30.89964   ]\n",
      " [ -0.72742486]] - fc1/kernel:0\n",
      "[-27.211784] - fc1/bias:0\n",
      "[[-159.53268]] - fc_out/kernel:0\n",
      "[214.51302] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 100\n",
      "xs\n",
      "[[ 1.          0.89100224  0.75377876  0.63418704  0.49726033  0.36295062\n",
      "   0.21946104  0.07333399 -0.07949694 -0.23697719 -0.40060037]\n",
      " [ 1.         -0.70999783  0.2878842  -0.29307607  0.03828476 -0.15777783\n",
      "  -0.04918584 -0.11683501 -0.08280326 -0.1077188  -0.09875493]]\n",
      "\n",
      "us\n",
      "[[-0.1099978  -0.13811451 -0.12034553 -0.13756089 -0.13480698 -0.14385255\n",
      "  -0.14634651 -0.15290427 -0.15740076 -0.16338621 -0.16881949]]\n",
      "\n",
      "loss\n",
      "[[5.375085]]\n",
      "\n",
      "gradients\n",
      "[[-0.24051097]\n",
      " [ 0.04955651]] - fc1/kernel:0\n",
      "[0.15035263] - fc1/bias:0\n",
      "[[-3.7769015]] - fc_out/kernel:0\n",
      "[-1.0845208] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 200\n",
      "xs\n",
      "[[ 1.          0.8730024   0.7360296   0.6047959   0.46761394  0.3313823\n",
      "   0.19198021  0.05181947 -0.0905614  -0.23432796 -0.3800073 ]\n",
      " [ 1.         -0.72799766  0.29895282 -0.3113415   0.04901817 -0.16611016\n",
      "  -0.0400674  -0.11631229 -0.07264533 -0.10008878 -0.08539173]]\n",
      "\n",
      "us\n",
      "[[-0.12799764 -0.13784578 -0.1319698  -0.13778673 -0.13669926 -0.1397335\n",
      "  -0.14035273 -0.1424327  -0.14367598 -0.145445   -0.14693235]]\n",
      "\n",
      "loss\n",
      "[[5.2373533]]\n",
      "\n",
      "gradients\n",
      "[[-0.07493743]\n",
      " [ 0.02219688]] - fc1/kernel:0\n",
      "[0.04025013] - fc1/bias:0\n",
      "[[-3.3610795]] - fc_out/kernel:0\n",
      "[-0.89933205] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 300\n",
      "xs\n",
      "[[ 1.          0.8565753   0.7204625   0.5787563   0.44240975  0.3047629\n",
      "   0.1698378   0.03517811 -0.09774323 -0.2298362  -0.36056542]\n",
      " [ 1.         -0.74442476  0.3096854  -0.32823798  0.06001753 -0.17409983\n",
      "  -0.03076998 -0.11636755 -0.06313599 -0.09411362 -0.0740312 ]]\n",
      "\n",
      "us\n",
      "[[-0.14442474 -0.13696946 -0.14242673 -0.13692527 -0.13808931 -0.13522989\n",
      "  -0.13482954 -0.13295652 -0.13199522 -0.13049938 -0.12934595]]\n",
      "\n",
      "loss\n",
      "[[5.130537]]\n",
      "\n",
      "gradients\n",
      "[[ 0.06534109]\n",
      " [-0.02534133]] - fc1/kernel:0\n",
      "[-0.03017496] - fc1/bias:0\n",
      "[[-2.962979]] - fc_out/kernel:0\n",
      "[-0.74411845] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 400\n",
      "xs\n",
      "[[ 1.          0.84180844  0.706944    0.55589426  0.4211338   0.2824023\n",
      "   0.15207158  0.02232803 -0.10214879 -0.22440486 -0.3427022 ]\n",
      " [ 1.         -0.75919163  0.31980872 -0.34364197  0.07086879 -0.18167391\n",
      "  -0.02160878 -0.11693036 -0.05434094 -0.08954935 -0.06434335]]\n",
      "\n",
      "us\n",
      "[[-0.15919162 -0.13570629 -0.15175672 -0.1353164  -0.13915265 -0.13061313\n",
      "  -0.12989563 -0.12449916 -0.12215391 -0.11807296 -0.11521394]]\n",
      "\n",
      "loss\n",
      "[[5.048514]]\n",
      "\n",
      "gradients\n",
      "[[ 0.18061976]\n",
      " [-0.08680665]] - fc1/kernel:0\n",
      "[-0.07154485] - fc1/bias:0\n",
      "[[-2.5923004]] - fc_out/kernel:0\n",
      "[-0.61658] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 500\n",
      "xs\n",
      "[[ 1.          0.8287021   0.69526386  0.53594327  0.40322387  0.2636292\n",
      "   0.13781548  0.01237859 -0.10462664 -0.21863818 -0.32658437]\n",
      " [ 1.         -0.772298    0.3291118  -0.35748303  0.08123448 -0.18873861\n",
      "  -0.01283418 -0.1178742  -0.04629309 -0.08613107 -0.05604889]]\n",
      "\n",
      "us\n",
      "[[-0.17229798 -0.134267   -0.16001591 -0.13325535 -0.13999793 -0.12607735\n",
      "  -0.12557471 -0.11701761 -0.11390692 -0.10772754 -0.1038247 ]]\n",
      "\n",
      "loss\n",
      "[[4.985853]]\n",
      "\n",
      "gradients\n",
      "[[ 0.27347153]\n",
      " [-0.15684333]] - fc1/kernel:0\n",
      "[-0.0924587] - fc1/bias:0\n",
      "[[-2.254747]] - fc_out/kernel:0\n",
      "[-0.51307964] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 600\n",
      "xs\n",
      "[[ 1.          0.817191    0.68517876  0.51859426  0.3881321   0.24783623\n",
      "   0.1263302   0.00462522 -0.10580575 -0.21291685 -0.31221414]\n",
      " [ 1.         -0.78380907  0.337456   -0.3697433   0.09086519 -0.19520313\n",
      "  -0.004632   -0.1190521  -0.03900434 -0.08360268 -0.04892274]]\n",
      "\n",
      "us\n",
      "[[-0.18380906 -0.13282946 -0.16726968 -0.13098079 -0.14068401 -0.12175388\n",
      "  -0.12183131 -0.11043561 -0.10700529 -0.09908435 -0.09459826]]\n",
      "\n",
      "loss\n",
      "[[4.9379435]]\n",
      "\n",
      "gradients\n",
      "[[ 0.34730223]\n",
      " [-0.23104456]] - fc1/kernel:0\n",
      "[-0.09952956] - fc1/bias:0\n",
      "[[-1.9531126]] - fc_out/kernel:0\n",
      "[-0.4298451] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 700\n",
      "xs\n",
      "[[ 1.          0.80716467  0.6764428   0.50352836  0.3753586   0.23449618\n",
      "   0.11700653 -0.00147472 -0.10613897 -0.20746113 -0.29949504]\n",
      " [ 1.         -0.7938354   0.34477216 -0.38045418  0.09959923 -0.20099737\n",
      "   0.00287426 -0.12032281 -0.03246909 -0.08173456 -0.0427857 ]]\n",
      "\n",
      "us\n",
      "[[-0.1938354  -0.13152911 -0.1735909  -0.12867329 -0.14123783 -0.11772417\n",
      "  -0.11859825 -0.10466278 -0.10121602 -0.09182644 -0.08706947]]\n",
      "\n",
      "loss\n",
      "[[4.901006]]\n",
      "\n",
      "gradients\n",
      "[[ 0.4055564 ]\n",
      " [-0.30593818]] - fc1/kernel:0\n",
      "[-0.09765225] - fc1/bias:0\n",
      "[[-1.6881218]] - fc_out/kernel:0\n",
      "[-0.3633287] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 800\n",
      "xs\n",
      "[[ 1.          0.7984841   0.6688238   0.49043354  0.36446446  0.22316007\n",
      "   0.10935263 -0.00633369 -0.10594504 -0.20238236 -0.2882774 ]\n",
      " [ 1.         -0.802516    0.35105076 -0.38968956  0.10735419 -0.20608139\n",
      "   0.00961822 -0.12156661 -0.02666505 -0.08033233 -0.03749326]]\n",
      "\n",
      "us\n",
      "[[-0.20251599 -0.13045883 -0.17905912 -0.12645955 -0.14166887 -0.11403062\n",
      "  -0.11579568 -0.09960502 -0.09633136 -0.08569266 -0.08086856]]\n",
      "\n",
      "loss\n",
      "[[4.872033]]\n",
      "\n",
      "gradients\n",
      "[[ 0.45133978]\n",
      " [-0.37894058]] - fc1/kernel:0\n",
      "[-0.09033822] - fc1/bias:0\n",
      "[[-1.4589554]] - fc_out/kernel:0\n",
      "[-0.31048656] - fc_out/bias:0\n",
      "----------------------\n",
      "training iteration 900\n",
      "xs\n",
      "[[ 1.          0.79099584  0.6621127   0.47901583  0.35507458  0.21345043\n",
      "   0.10297793 -0.01026237 -0.1054424  -0.19772002 -0.27838683]\n",
      " [ 1.         -0.81000423  0.35632846 -0.39755613  0.11411339 -0.21044728\n",
      "   0.01558241 -0.12269273 -0.02155413 -0.0792397  -0.03292528]]\n",
      "\n",
      "us\n",
      "[[-0.21000424 -0.1296741  -0.18375903 -0.1244203  -0.14197925 -0.11068596\n",
      "  -0.11334328 -0.09516977 -0.09217218 -0.0804691  -0.07570297]]\n",
      "\n",
      "loss\n",
      "[[4.8486958]]\n",
      "\n",
      "gradients\n",
      "[[ 0.48728493]\n",
      " [-0.44828144]] - fc1/kernel:0\n",
      "[-0.08002496] - fc1/bias:0\n",
      "[[-1.2636743]] - fc_out/kernel:0\n",
      "[-0.26877427] - fc_out/bias:0\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        sess.run(train, feed_dict={input_layer : x0.T})\n",
    "        if i % 100 == 0:\n",
    "            results = sess.run([xs, us, loss], feed_dict={input_layer : x0.T})\n",
    "            labels  = \"xs us loss\".split(' ')\n",
    "            print('training iteration', i)\n",
    "            for label,result in zip(*(labels,results)) :\n",
    "                print(label)\n",
    "                print(result)\n",
    "                print('')\n",
    "            print('gradients')\n",
    "            for g, v in grads_and_vars:\n",
    "                print(str(sess.run(g, feed_dict={input_layer : x0.T})) + \" - \" + v.name)\n",
    "            print('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## compute optimal loss with true LQR ricatti equation formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import controlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    A_np, B_np, Q_np, R_np = sess.run([A, B, Q, R])\n",
    "\n",
    "K, P, eig = controlpy.synthesis.controller_lqr_discrete_time(A_np, B_np, Q_np, R_np)\n",
    "x_np = x0\n",
    "u_np = (-K@x_np)\n",
    "xs_np, us_np = np.array(x_np), np.array(u_np)\n",
    "loss_np = 0\n",
    "for i in range(T):\n",
    "    loss_np += x_np.T@Q_np@x_np + u_np.T*R_np*u_np\n",
    "    x_np = A_np@x_np + B_np@u_np\n",
    "    u_np = (-K@x_np)\n",
    "    xs_np = np.hstack([xs_np, x_np])\n",
    "    us_np = np.hstack([us_np, u_np])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.71948966]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling iter_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/nishant/ray_results/2018-08-26_22-43-29r4a9tt7u/iter_vars.pkl', 'rb') as file:\n",
    "    itervars = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grad': array([-0.15219422,  0.90641534, -0.0200394 ,  0.5127883 ,  0.25791866],\n",
       "       dtype=float32),\n",
       " 'weights': {'fc1/biases': array([0.01161656], dtype=float32),\n",
       "  'fc1/weights': array([[-1.086187  ],\n",
       "         [-0.03679204]], dtype=float32),\n",
       "  'fc_out/biases': array([-0.03084663], dtype=float32),\n",
       "  'fc_out/weights': array([[0.38942182]], dtype=float32)}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itervars[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[ 3.2057407]\\n [-3.6136274]] - fc1/kernel:0\\n[3.6281395] - fc1/bias:0\\n[[-4.8025885]] - fc_out/kernel:0\\n[-4.03341] - fc_out/bias:0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"[[ 3.2057407]\n",
    " [-3.6136274]] - fc1/kernel:0\n",
    "[3.6281395] - fc1/bias:0\n",
    "[[-4.8025885]] - fc_out/kernel:0\n",
    "[-4.03341] - fc_out/bias:0\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weights(w):\n",
    "    nw = dict()\n",
    "    nw['fc1/kernel'] = w['fc1/weights']\n",
    "    nw['fc1/bias'] = w['fc1/biases']\n",
    "    nw['fc_out/kernel'] = w['fc_out/weights']\n",
    "    nw['fc_out/bias'] = w['fc_out/biases']\n",
    "    return nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients\n",
      "[[ 1.3710754 ]\n",
      " [-0.10271908]] - fc1/kernel:0\n",
      "[1.5390728] - fc1/bias:0\n",
      "[[-61.29186]] - fc_out/kernel:0\n",
      "[78.44572] - fc_out/bias:0\n",
      "gradients\n",
      "[[ 1.4524152 ]\n",
      " [-0.15942253]] - fc1/kernel:0\n",
      "[1.7677462] - fc1/bias:0\n",
      "[[-43.78142]] - fc_out/kernel:0\n",
      "[61.0312] - fc_out/bias:0\n",
      "gradients\n",
      "[[ 1.2619764 ]\n",
      " [-0.19968915]] - fc1/kernel:0\n",
      "[1.6559447] - fc1/bias:0\n",
      "[[-29.626226]] - fc_out/kernel:0\n",
      "[44.950897] - fc_out/bias:0\n",
      "gradients\n",
      "[[ 1.011159  ]\n",
      " [-0.22799984]] - fc1/kernel:0\n",
      "[1.3879302] - fc1/bias:0\n",
      "[[-19.05171]] - fc_out/kernel:0\n",
      "[30.772741] - fc_out/bias:0\n",
      "gradients\n",
      "[[ 0.72370774]\n",
      " [-0.23511848]] - fc1/kernel:0\n",
      "[0.95035666] - fc1/bias:0\n",
      "[[-11.422196]] - fc_out/kernel:0\n",
      "[18.386967] - fc_out/bias:0\n"
     ]
    }
   ],
   "source": [
    "for iteration in itervars[:5]:\n",
    "    eval_grads(process_weights(iteration['weights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
